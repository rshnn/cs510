# cs510 - lecture notes
>Tue 20 Feb 2018 06:46:32 PM EST

```
f(x) = x^2-2
```

Has roots (+-sqrt(2)) and a minimum (at x=0).
+ Can use root finding (Newton's) methods to find the roots  
  
Another way is what we went over last time.   
Do this thing: 

\alpha f(x) = 0
x - \alpha f(x) = x             \alpha = learning rate  
x - \alpha f(x) = g(x) = x  
abs( g'(x))<1 , [a,b]  
+ Gives you the values of \alpha where this method will work.  


Lets use this method for x^2-2

```
x=1  
>> x=x-0.1 * (x^2-2)  
x = 1.17...
```
 
x = 1.24... 
x = 1.32... 
x = 1.34... 
x = 1.4107... 


Should converge to sqrt(2).  

This is basically a gradient descent/ascend method.   Depending on where you start its ascending or descending.  By starting at 1, we are ascending to the higher root (at sqrt(2)).  



Going back to the absolute value inequality thing: 

```
-1 < | 1 - alpha*2*sqrt(2) | < 1
-2 < - alpha 2 sqrt(2) < 0
1/sqrt(2) > alpha > 0
```


This points towards the best value for alpha... (alpha ~ 0.35)
+ quadratic methods have g'(x) at derivative = 0.  
+ here we let g'(x) be zero and solve for alpha.  This way, this method converges quadratically.   
+ When both the first and second derivatives are zero, it is cubic.   

x = x - \alpha * (2x)  

For this to converge... 

| g'(x) | < 1
| 1 - 2*alpha| < 1
-1 < 1 - alpha 2 < 1
1/2 > alpha > 0


```
x = x - a* (2*x)
// converges linearly towards the minmium.  Can change this if you use the right learning parameter.  
```

alpha = 1/2
//  this is from g'(x) = 0.   1 - 2*alpha = 0 --->  alpha = 1/2  



--- 

Lets do another function..  The sine function.  

x = x - a * (cos(x))
g(x) = x - a*cos(x)
g'(x) = 1 + a*sin(x) 


x = 3pi/2

g'(x) = 1 + a sin(3pi/2)
g'(x) = 1 + a (-1) = 1-a

|1-a| < 1
-1 < 1-a < 1
1 > a-1 > -1
2 > a > 0  


```matlab 
x = pi/2 + 0.2 
a = 0.4

x = x - a*cos(x)
```

The above should descend to the minimum at 3pi/2 (4.71...).  

--- 

For multidimensional problems, Newtons method gets complicated.  
This method scales well to multi-dimensions (the steepest descend method).  

vect(x) - alpha * (del)f(vect(x)) = x  

(del)f --> the gradient (partial derivatives)  


--- 

## Least squares method  

Dealing with the prediction problem based on a set of sample data.  
We want a function that fits the data and use it as an approximation to predict behavior.  


How do we optmize this process?  How do we get the best line of fit?  
We need to define what 'best' means...  

We define error.  

err = f(x_i) - y_i
The difference between the predicted value and the value in the data.  
This way we can compare different approximation functions against one another.  

+ min max rule
    * Minimize the largest error.  
+ Or we can work with summations:
    * min sum_i=1^n {abs(error_i)^2}
    * min sum {error_i}

+ three methods listed in 7-1   


>Tue 20 Feb 2018 08:04:04 PM EST
BREAK 

>Tue 20 Feb 2018 08:11:49 PM EST
RETURN 


---
Going over 7-2 noteset.  
-> Least Squares Regression  
---

Have a bunch of points (x_i, y_i). 
f(x) = a*x+b  is our approximating function.  
```
G(a,b) = SUM_i=1^m{(f(x_i) - y_i)^2} = SUM_i=1^m{(a*x_i + b -y_i)^2}
```

(del)G = 0  --> SAME AS --> (dG/da, dG/db) = 0  
Above is partial derivatives.    
```
dG/da = SUM {2(ax_i+b-y_i)*(x_i)}  = 0  
        = 2*a*SUM{x_i^2} + 2*b*SUM{x_i} = 2SUM{x_i*y_i}
```


```
dG/db = SUM {2(ax_i + b - y_i)*(1)} = 0  
        = 2*a*SUM{x_i} + 2b SUM{1} = 2SUM{y_i}
```


This is a system of equations.  Can represent it in matrix form as follows:  

```
| SUM(1)        SUM(x_i)    |       |   a   |  =    |  SUM(y_i)       |
| SUM(x_i()     SUM(x_i^2)  |       |   b   |       |  SUM(x_i*y_i)   |

```




---

A = [e_n  &&  x_n]
i.e.  ones vector + x vector. 

A^T * A * [a \\ b]
--- 


The above is the same as the big ass matrix formula shown before.  
```
A'Ac = A'y  
```
where x = [b // a],  
A = [ones(m) && x_vector(m)]  

**Called normal equations**.

--- 
Doing examples... 

Looking at f(x) = x^2-2.  


A =  
|  1   -sqrt(2) |     
|  1   0        |      
|  1   sqrt(2)  | 
|  ..   ..      |     
|  1    x_m     | 

A'Ac = [1, 1, 1; -sqrt(2), 0, sqrt(2)]*[0; -2; 0]  
[3, 0; 0, 4] * [b; a] = [-2; 0]  
3b = -2
    -> b = -2/3 
4a = 0
    -> a = 0


y = -2/3.  is the line of best fit.


What it he total error with three points?  (the two roots and the min). 


error = (2/3)^2 + (2-2/3)^2+(2/3)^2 = 24/9 = 2.66666666667   


--- 

We were trying to fit using this function: 

f(x) = a x + b

Lets be more general about it..  

f(x) = a_1+phi_1(x) + a_2*phi_2(x) + ... + a_n*phi_n(x)
    = SUM_i=1^n{a_i*phi_i(x)}



Error = SUM{(f(x_i)-y_i)^2}     // the same.  

  
```
A'Ac = A'y

A = 
| phi_1(x_1),  phi_2(x_1) ... phi_n(x_1) |  
| phi_1(x_2),  phi_2(x_2) ... phi_n(x_2) |  
| phi.1(x_.),  phi_2(x_.) ... phi_n(x_.) |  
| phi_1(x_m),  phi_2(x_m) ... phi_n(x_m) |  

```


Example: 

```
A = [1, x_i,x_i^2]
A = 
[1, -sqrt(2), 2; 
-sqrt(2), 0, sqrt(2); 
2, 0, 2]

c = [a_1; a_2; a_3]

y = [0; -2; 0]

A'Ac=A'y

...do math...

a_1 = -2
a_2 = 0
a_3 = 1
```


---

G(a_1, a_2, a_3) =  SUM_i=1^m {  SUM_j=1^n{ (a_j*phi_j(x_i)-y_i)^2 }}

Want to minimize this.   So take its del... 

(del)G = 0.  



```
dG/da_k = SUM_i=1^n{2[SUM{a_j*phi_j(x_i}-y_i]*phi_k(x_i)} = 0  


= 2 SUM_i{SUM_j{a_j*phi_j*phi_k - y_i*phi+k}} = 0
= SUM_i{SUM_j{a_j*phi_j*phi_k}} - SUM_i{y_i*phi_k(x_i)} = 0  

SUM_j{a_j*SUM_i{phi_j(x_i)*phi_k(x_i)}} = SUM_j{y_i*phi_k(x_i)}  

```
Can either use the above or the A matrix thing from before.   









































