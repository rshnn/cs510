# lecture 
>Tue 27 Feb 2018 06:45:05 PM EST


## Going over Homework 3 

We learned two types of polynomials.  LaGrange and Newtons polynomials.   They are the same xcept computed differences.  

Also learned least squares. 
+ Use polynomial of lower degree, attempt to minimize error 
+ Better for noisy data  
+ Same as interpolation polynomial with n-1.  Because error is zero.
    * But note that it doesnt scale well to other problems.  overfits.  



1.  Have this function f(x).. We have a program that interpolates it.  

```matlab

function runge(n)
%Function that shows the Runge phenomena
%
% The Polynomial interpolation of f(x) =1/(1 +10 * x^2)
% on equal distributed n nodes on [-1 1]
% causes extreme variation near the ends
% as n increases
%INPUT n the # of equally distributed nodes in [-1 1]
%
%
close all
x=-1:0.02:1; % mesh points on [-1 1] used for plotting

% Doing element-wise division.   
y=1./(1 + 10 * x.^2);

% just plot to see the function.  
plot(x,y) % a plot of f


% Get evenly spaced set of points on the function.  This builds the input dataset.     
xp=linspace(-1,1,n); % the interpolation nodes
yp=1./(1 + 10 * xp.^2); %y_i=f(x_i)
hold on
plot(xp,yp,'o') %plotting the data points on the graph of f



C=polyfit(xp,yp,n-1); %computes the coefficients of P_{n-1}
Interp=polyval(C,x); %evaluates the Interpolant P_{n-1} at the mesh points
plot(x,Interp,'r')
end

```

Missed some tanget conversations...


4.  The f^(n+1) is much larger than the other term.


```
syms x
z = diff('1/(1+10*x^2', 11)
subs(z, x, 3.7)
```

Use the previous script to plot this and test out what the error looks like at the ends.  
Note that x=4 is an endpoint for the above.   





## MATLAB 

Going over useful tools in the matlab toolkit. 
* Went over `poly23`  
* Exponential models  
+ There are tutorials available for more interpolation methods in mathworks site [i think this is the link](https://www.mathworks.com/help/curvefit)
    * polynomial models
    * exponential models
    * fourier series 
    * etc 

## Discussing Machine Learning a bit

- discussing overfitting.  
- splitting training data 80/20
    + leave-one-out  
* Overfitting is a big issue nowadays   


## The polynomial/fourier fit functions are the same as least-squared 

The point is to minimize the least squared error.  There is still a phi_i(x), they are just different between the methods.  



## Starting new section.  Chapter 6--Linear Systems 

Going over 6-1.  

Discussing the importance of linear algebra.  Critical for modern machine learning.  