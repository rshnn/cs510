# cs510 - lecture notes
>Tue 13 Feb 2018 06:38:02 PM EST
 

Question about the homework.  
[Halley method](https://en.wikipedia.org/wiki/Halley%27s_method)
+ a cubic convergent method  
+ has Newton's method within it   
+ faster in terms of convergence, but quadratic is good enough
    * More computations per step  


Reviewing fixed-point iteration.
+ We are looking for roots: f(x) = 0.
    * In fixed-point, we take x=g(x) and x_(n+1) = g(x_n)
+ Newton's method is just a special case of fixed-point iteration where g(x) is 
```
g(x) = x - (f(x)/f'(x)) 
```


## [Steepest descent](http://mathworld.wolfram.com/MethodofSteepestDescent.html)
```
x-\alpha f'(x) = 0 + x
```
+ popular ML method
+ \alpha is called the learning rate 
+ elegant in multi-dimension.  generalizes well 
    * `x - \alpha \gradiant_f = x`  
+ Derivation is super simple... just add x to both sizes and a scaling factor (\alpha)   
  
Okay.  Let's do an example.  

Suppose we want to solve `f(x) = x^2-2`.  

`x_(n+1) = x_n - \alpha (x_n^2-2)`
  
`g(x) = x - \alpha (x^2-2) `  
`|g'(x)| = |1-\alpha (2 sqrt(2)) < 1|`    # derivative must be less than 1  
`\alpha < 1/sqrt(2)`
  
This is saying that it will converge for those values of \alpha.  

```matlab
a = sqrt(2)
> a = whatever the exact solution is in long precision  
> # This is the root of the f(x) above.  


a = 0.5
x=0
x=x-a*(x^2-2)
> # this is the steepest descent iterate   
> # repeat this.  it will keep converging.  It goes back and forth around the true root val.  
        
```

Anyway, the most important thing about this method is it's ability to generalize to higher dimensions easily.  Its dope.       



## "Why am i using so much math ahhrgh"

Applications in deep learning.  An aside on voice regonition, facial recognition, Apple's latest ML chip, etc.  All very recent..after 2012-ish.  

These are all a combination of accomplishments of numerical methods and better processing power.   The ML chip is basically just a tiny graphics card thing that can do matrix operations really quickly.  

The steepest descent method is within these applications.  
You cannot avoid learning it.  It is everywhere.  



## Back to interpolation  

Given a set of data, can you give me a function that goes through this data?   
Want a good, smooth, well fitting function.  Polynomial functions behave nicely, so they are preferred..   
 
Interpolation method can become Taylor's method if you take the limit onto one point.  

>Tue 13 Feb 2018 07:25:33 PM EST

Walking through the sec_4-1 notes.   

Highlighting that the linear interpolation method on page 6 has two forms:
+ Lagrange form (first line) @ P(x) = .. ) and newtons form (last line)  
+ They are computationally different    

Quadratic interpolation  
+ Note the Lagrange polynomials.
    + For L_0.  if you plug in x_0, you get 0.  For all else, you get 1.   
    + Refer to the L_k(x) equation.  It skips (x-x_k).    


### divided difference 

Differences are like derivatives, cept derivatives are taking the limit of the difference.  

Going through how the **difference** is calculated.  

Look at general method on page 28  
* This is taylor's method expanded to more than one point.  
+ Thus, this method reduces to taylor's method for one point.  

Recall the difference table.  
+ This is an advantage of this method because the table is easy to make.  

Nested multiplication  (page 30)
+ Makes things more computationally efficient.  
+ Every step is one multiplication and one addition if you start from the inner most terms.  Only n multiplications.  



Lets do an exmaple.  


`f(x) = x^3 - x - 1`


| x | f(x) | f[,] | f[,,] | f[,,,] | f[,,,,]   |
|---|------|------|-------|--------|-----------|
| 0 | -1   | 0    | 3     | 1      | 0 (error) |
| 1 | -1   | 6    | 6     | 1      |           |
| 2 | 5    | 18   | 9     |        |           |
| 3 | 23   | 36   |       |        |           |
| 4 | 59   |      |       |        |           |



Got 0 error because f(x) can be interpolated with a polynomial degree 3 function with 0 error.  The last column represents the 4th derivative of the interpolated function, which is 0 (think of Taylor's method).    


```
P_4(x) = -1 + 0(x-0)+3(x-0)(x-1) + 1(x-0)(x-1)(x-2) + 0(x-0)(x-1)(x-2)(x-3)
        = x^3-x-1
```


>Tue 13 Feb 2018 08:04:07 PM EST
Break  



>Tue 13 Feb 2018 08:30:10 PM EST
Return 

## Returning to gradient descent again for a bit   
Googling some stuff.  Finding pictures from high dimensional problems.  
+ [just looked at the photo](http://blog.datumbox.com/tuning-the-learning-rate-in-gradient-descent/)


+ Prof is working on a genre recognition thing. 
    + http://aurora.cs.rutgers.edu:8080/gramusik_v1/index.php/songs
    + Listening to jams from around the world.  
    + It is a deep learning project 


## Interpolation errors 

Section 4-2.  



```
P_1(x) = f(x_0) = (x-x_0)f[x_0, x_1]  
```

Error is: `f(x) - P_n(x) = f(x) - f(x_0) - (x-x_0)f[x_0, x_1]`

  
`f(x) = P_infinity(x) = f(x_0) + f[x_0, x_1](x-x_0) + f[x_0, x_1, x_2](x-x_0)(x-x_1 + ... + f[x_0, ..., x_n](x-x_0)...(x-x_n)`  

Note that the first two terms are equivalent to P_1(x).   All the terms afterwards represent the error (a series into infinity).  


Error can be approximated by the first term of this thing.  
```
R_n(x) = f[x_0, x_1, ..., x_n, \zeta] \product_(i=0)^n (x-x_i)    #the difference form

(f^(n+1) (\eta) / (n+1)! ) \product_(i=0)^n (x-x_i)    # approximate form 
```

This is not exactly the error, but it is proportional to it and has the same behavior as the true error.   



Going through the 4-2 note set.  

### Runge's Example -- page 22

```MATLAB
> syms x
> diff(1/(1+x^2), 2)
> subs(ans, x, 1)
>       ans = 1/2
>       
> 
> diff(1/(1+x^2), 10)
> subs(ans, x, 5)
>       ans = something-huge  # example of bad interpolation 
```

When the derivatives get large, the approximations usually misbehave.  The error is proportional to the derivative.  

Corners are bad news bears for derivatives.  A tactic against this is to put more sample points around these rough corner spots so that interpolation around these points are a smoother fit.  

The engineers of the 60s/70s developed spline methods to help with this.  
+ Put lower polynomial interpolations in some areas are higher polynomial interpolations in others.  It is a piece-wise function.  You do not try to approximate the entire interval with one polynomial approximation.  The boundaries of the piece-wise functions have the same derivative so that the overall spline is smooth.  

d
>Tue 13 Feb 2018 09:31:32 PM EST

<\lecture>


